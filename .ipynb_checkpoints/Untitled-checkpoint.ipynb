{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatTool import *\n",
    "import pickle\n",
    "import random\n",
    "LangBag = \"dict.pkl\"\n",
    "DataName = \"./dgk_lost_conv/results/lost.conv.tconv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LangBag, 'rb') as f:\n",
    "    lang = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4411"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lang.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Reader(DataName)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, \n",
    "                                              shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, em_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.em_size = em_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, self.em_size)\n",
    "        self.gru = nn.GRU(self.em_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.embedding(input.long()).view(1, -1, self.em_size)\n",
    "        output, hidden = self.gru(output)\n",
    "        output = self.out(output[:,-1,:])\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, em_size, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.em_size = em_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, em_size)\n",
    "        self.gru = nn.GRU(em_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input.long()).view(1, -1, self.em_size)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output[0], hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTSIZE = lang.n_words\n",
    "encoder = EncoderRNN(INPUTSIZE, 64, 256)\n",
    "decoder = DecoderRNN(INPUTSIZE, 64, 256, INPUTSIZE)\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 8.319211174460019\n",
      "Step 10, loss: 7.7915596008300785\n",
      "Step 20, loss: 6.918454306466239\n",
      "Step 30, loss: 6.955694410536024\n",
      "Step 40, loss: 3.337294006347656\n",
      "Step 50, loss: 6.4293401808965776\n",
      "Step 60, loss: 4.884124755859375\n",
      "Step 70, loss: 4.4417500495910645\n",
      "Step 80, loss: 4.114537920270648\n",
      "Step 90, loss: 4.008284160069057\n",
      "Step 100, loss: 5.026163437787225\n",
      "Step 110, loss: 5.096636090959821\n",
      "Step 120, loss: 5.174397277832031\n",
      "Step 130, loss: 5.715472171181126\n",
      "Step 140, loss: 5.837334576775046\n",
      "Step 150, loss: 9.253380584716798\n",
      "Step 160, loss: 5.027745056152344\n",
      "Step 170, loss: 6.156867394080529\n",
      "Step 180, loss: 4.812440490722656\n",
      "Step 190, loss: 4.24296047813014\n",
      "Step 200, loss: 5.25107659233941\n",
      "Step 210, loss: 4.710901472303602\n",
      "Step 220, loss: 3.649584633963449\n",
      "Step 230, loss: 4.921427986838601\n",
      "Step 240, loss: 5.4434384946469905\n",
      "Step 250, loss: 5.483907699584961\n",
      "Step 260, loss: 4.650739351908366\n",
      "Step 270, loss: 6.360753195626395\n",
      "Step 280, loss: 5.2237772941589355\n",
      "Step 290, loss: 6.4089884757995605\n",
      "Step 300, loss: 5.89787351168119\n",
      "Step 310, loss: 4.6416457039969305\n",
      "Step 320, loss: 4.372399393717448\n",
      "Step 330, loss: 4.095490137736003\n",
      "Step 340, loss: 5.653017520904541\n",
      "Step 350, loss: 4.930106692843967\n",
      "Step 360, loss: 4.076651891072591\n",
      "Step 370, loss: 5.236902757124468\n",
      "Step 380, loss: 4.163000742594401\n",
      "Step 390, loss: 6.557427799000459\n",
      "Step 400, loss: 3.4980897903442383\n",
      "Step 410, loss: 4.821607317243304\n",
      "Step 420, loss: 5.137167150324041\n",
      "Step 430, loss: 4.448140716552734\n",
      "Step 440, loss: 4.93242917742048\n",
      "Step 450, loss: 7.102376937866211\n",
      "Step 460, loss: 5.580821514129639\n",
      "Step 470, loss: 5.862815856933594\n",
      "Step 480, loss: 5.099608201246995\n",
      "Step 490, loss: 3.993599700927734\n",
      "Step 500, loss: 5.147225189208984\n",
      "Step 510, loss: 5.591256921941584\n",
      "Step 520, loss: 3.9643427530924478\n",
      "Step 530, loss: 5.061773935953776\n",
      "Step 540, loss: 6.785109837849935\n",
      "Step 550, loss: 5.903691291809082\n",
      "Step 560, loss: 5.025064786275228\n",
      "Step 570, loss: 4.509676933288574\n",
      "Step 580, loss: 5.357921600341797\n",
      "Step 590, loss: 3.525506155831473\n",
      "Step 600, loss: 4.962491226196289\n",
      "Step 610, loss: 4.812615645559211\n",
      "Step 620, loss: 4.255087908576517\n",
      "Step 630, loss: 4.630356258816189\n",
      "Step 640, loss: 3.139950616019113\n",
      "Step 650, loss: 5.291597366333008\n",
      "Step 660, loss: 5.619412422180176\n",
      "Step 670, loss: 3.8776461283365884\n",
      "Step 680, loss: 5.096831851535374\n",
      "Step 690, loss: 4.4206695556640625\n",
      "Step 700, loss: 2.4082942962646485\n",
      "Step 710, loss: 4.972559247698102\n",
      "Step 720, loss: 2.7653811318533763\n",
      "Step 730, loss: 3.492970206520774\n",
      "Step 740, loss: 5.3167572021484375\n",
      "Step 750, loss: 5.508132510715061\n",
      "Step 760, loss: 3.836997477213542\n",
      "Step 770, loss: 5.0581817626953125\n",
      "Step 780, loss: 4.757153828938802\n",
      "Step 790, loss: 6.028976864284939\n",
      "Step 800, loss: 4.086463928222656\n",
      "Step 810, loss: 4.452582550048828\n",
      "Step 820, loss: 3.8631065913609097\n",
      "Step 830, loss: 5.325239354913885\n",
      "Step 840, loss: 3.5894108822471216\n",
      "Step 850, loss: 3.8608531951904297\n",
      "Step 860, loss: 1.725496482849121\n",
      "Step 870, loss: 4.7997117884018845\n",
      "Step 880, loss: 6.420813340407151\n",
      "Step 890, loss: 2.959809494018555\n",
      "Step 900, loss: 4.345387352837457\n",
      "Step 910, loss: 4.305421193440755\n",
      "Step 920, loss: 4.909668350219727\n",
      "Step 930, loss: 4.241407606336805\n",
      "Step 940, loss: 5.155776023864746\n",
      "Step 950, loss: 4.628908920288086\n",
      "Step 960, loss: 5.68880904348273\n",
      "Step 970, loss: 4.349569638570149\n",
      "Step 980, loss: 4.367373817845395\n",
      "Step 990, loss: 5.070788663976333\n",
      "Step 1000, loss: 4.497897556849888\n",
      "Step 1010, loss: 4.187876614657315\n",
      "Step 1020, loss: 4.586904048919678\n",
      "Step 1030, loss: 5.013663071852464\n",
      "Step 1040, loss: 4.451446056365967\n",
      "Step 1050, loss: 1.6615089416503905\n",
      "Step 1060, loss: 4.805454254150391\n",
      "Step 1070, loss: 5.407802581787109\n",
      "Step 1080, loss: 4.370348612467448\n",
      "Step 1090, loss: 3.0748777389526367\n",
      "Step 1100, loss: 3.793145986703726\n",
      "Step 1110, loss: 3.908939179920015\n",
      "Step 1120, loss: 3.2450485229492188\n",
      "Step 1130, loss: 6.928983052571614\n",
      "Step 1140, loss: 5.122239249093192\n",
      "Step 1150, loss: 5.241539001464844\n",
      "Step 1160, loss: 5.616552080426898\n",
      "Step 1170, loss: 6.216681586371528\n",
      "Step 1180, loss: 1.6857799530029296\n",
      "Step 1190, loss: 5.452810287475586\n",
      "Step 1200, loss: 5.858722686767578\n",
      "Step 1210, loss: 4.027819633483887\n",
      "Step 1220, loss: 7.9622240568462175\n",
      "Step 1230, loss: 4.939673832484654\n",
      "Step 1240, loss: 5.157845687866211\n",
      "Step 1250, loss: 4.438980551326976\n",
      "Step 1260, loss: 2.0528529030936107\n",
      "Step 1270, loss: 4.959057147686298\n",
      "Step 1280, loss: 4.9895782470703125\n",
      "Step 1290, loss: 4.5023088455200195\n",
      "Step 1300, loss: 4.66441890171596\n",
      "Step 1310, loss: 4.115718523661296\n",
      "Step 1320, loss: 3.7093922008167612\n",
      "Step 1330, loss: 4.614969162713914\n",
      "Step 1340, loss: 4.987289810180664\n",
      "Step 1350, loss: 6.058680447665128\n",
      "Step 1360, loss: 4.685074546120384\n",
      "Step 1370, loss: 5.438807983398437\n",
      "Step 1380, loss: 5.757699263723273\n",
      "Step 1390, loss: 4.384739215557392\n",
      "Step 1400, loss: 5.166325737448299\n",
      "Step 1410, loss: 3.7783966064453125\n",
      "Step 1420, loss: 4.676714810458097\n",
      "Step 1430, loss: 5.278117427119502\n",
      "Step 1440, loss: 4.151761690775554\n",
      "Step 1450, loss: 4.0760963439941404\n",
      "Step 1460, loss: 2.4066341400146483\n",
      "Step 1470, loss: 3.599869728088379\n",
      "Step 1480, loss: 3.0062158584594725\n",
      "Step 1490, loss: 5.122351797003495\n",
      "Step 1500, loss: 4.140684509277344\n",
      "Step 1510, loss: 2.961409568786621\n",
      "Step 1520, loss: 6.115452660454644\n",
      "Step 1530, loss: 5.157283359103733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ball/anaconda3/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ball/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1540, loss: 6.271239471435547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/ball/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ball/anaconda3/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ball/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ball/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ball/anaconda3/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8612a6f43cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ball/anaconda3/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i,data in enumerate(trainloader, 0):\n",
    "        input, target = data\n",
    "        input = torch.Tensor(lang.sentenceToVector(input[0])).long()\n",
    "        target = torch.Tensor(lang.sentenceToVector(target[0])).long()\n",
    "        if use_cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "        inputLen = input.size()[0]\n",
    "        targetLen = target.size()[0]\n",
    "\n",
    "        input = Variable(input)\n",
    "        target = Variable(target)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        o, encoder_hidden = encoder(input)\n",
    "        decoder_input = target[0]\n",
    "\n",
    "        decoder_hidden = encoder_hidden[:, -1, :].view(1,1,-1)\n",
    "\n",
    "        loss = 0\n",
    "        use_teacher_forcing = True if random.random() < 0.5 else False\n",
    "        for di in range(targetLen):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0]\n",
    "            decoder_input = target[di]\n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = Variable(torch.LongTensor([ni]))\n",
    "                if use_cuda:\n",
    "                    decoder_input = decoder_input.cuda()\n",
    "            else:\n",
    "                decoder_input = target[di]\n",
    "\n",
    "            loss += criterion(decoder_output, target[di])\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"Step {}, loss: {}\".format(i, loss.data[0] / targetLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3766\n",
      "3834\n",
      "3903\n",
      "2839\n",
      "402\n",
      "2643\n",
      "3903\n",
      "3987\n",
      "3505\n",
      "3338\n",
      "4026\n",
      "2839\n",
      "1805\n",
      "834\n",
      "2839\n",
      "402\n",
      "2643\n",
      "3903\n",
      "2964\n",
      "3561\n",
      "病人已經無法挽救了 我需要...\n",
      "蝨篷鴇慕總猛鴇鰥猿慍蝌慕寄武慕總猛鴇諱繚\n"
     ]
    }
   ],
   "source": [
    "q,a = trainset[5]\n",
    "\n",
    "input = torch.Tensor(lang.sentenceToVector(a)).long()\n",
    "input = Variable(input).cuda()\n",
    "o, encoder_hidden = encoder(input)\n",
    "\n",
    "decoder_hidden = encoder_hidden[:, -1, :].view(1,1,-1)\n",
    "decoder_input = input[0]\n",
    "ans = []\n",
    "for i in range(20):\n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0]\n",
    "    ans.append(ni)\n",
    "    decoder_input = Variable(torch.LongTensor([ni])).cuda()\n",
    "    print(ni)\n",
    "    if ni == 1:\n",
    "        break\n",
    "print(q, a)\n",
    "print(lang.vectorToSentence(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,a = trainset[0]\n",
    "ip = Variable(torch.Tensor(lang.sentenceToVector(q)).long()).cuda()\n",
    "op = Variable(torch.Tensor(lang.sentenceToVector(a)).long()).cuda()\n",
    "\n",
    "eo, eh = encoder(ip)\n",
    "\n",
    "oh = eh[:, -1, :].view(1,1,-1)\n",
    "for w in op:\n",
    "    ans, oh = decoder(op[0], oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = encoder.embedding(ip)\n",
    "o,h = encoder.gru(o.view(1, -1, encoder.em_size))\n",
    "o[:,-1,:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
